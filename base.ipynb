{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roster id's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2637, 23)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the primary train data\n",
    "train_path = './MergedTrainData.csv'  # Replace with the actual file path\n",
    "data = pd.read_csv(train_path)\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "# data_types = data.dtypes\n",
    "\n",
    "# print(data_types)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_cols = ['team1', 'team2', 'toss winner', 'toss decision', 'venue', 'city', 'lighting', 'series_name', 'season']\n",
    "for col in categorical_cols:\n",
    "    data[col] = label_encoder.fit_transform(data[col])\n",
    "\n",
    "# drop column\n",
    "data.drop(columns=['match_dt','winner'], inplace=True)\n",
    "data.drop(columns=['team1_roster_ids', 'team2_roster_ids'], inplace=True)\n",
    "\n",
    "# Display the updated data types\n",
    "# print(data.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = data.isnull().sum()\n",
    "# print(\"Missing Values:\\n\", missing_values)\n",
    "\n",
    "# Handle missing values if any\n",
    "# For simplicity, let's fill missing values with the mean of each column\n",
    "data.fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (2109, 18) (2109,)\n",
      "Validation set shape: (528, 18) (528,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# Define features (X) and target variable (y)\n",
    "X = data.drop(columns=['winner_id'])\n",
    "y = data['winner_id']\n",
    "\n",
    "# Split the dataset into training and validation sets (80% train, 20% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shapes of the training and validation sets\n",
    "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation set shape:\", X_val.shape, y_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 972 candidates, totalling 2916 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rahul.k.a.jha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rahul.k.a.jha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "972 fits failed out of a total of 2916.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "188 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\rahul.k.a.jha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\rahul.k.a.jha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\rahul.k.a.jha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\rahul.k.a.jha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "784 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\rahul.k.a.jha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\rahul.k.a.jha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\rahul.k.a.jha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\rahul.k.a.jha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\rahul.k.a.jha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1052: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.6230441  0.63205311 0.63584637\n",
      " 0.60739687 0.61640588 0.61830251 0.56330014 0.56614509 0.56804173\n",
      " 0.60312945 0.60787103 0.61308677 0.59601707 0.59649123 0.60075865\n",
      " 0.55903272 0.55761024 0.56282598 0.55429113 0.5637743  0.56235183\n",
      " 0.55429113 0.5637743  0.56235183 0.54812707 0.55476529 0.55713608\n",
      " 0.6230441  0.63205311 0.63584637 0.60739687 0.61640588 0.61830251\n",
      " 0.56330014 0.56614509 0.56804173 0.60312945 0.60787103 0.61308677\n",
      " 0.59601707 0.59649123 0.60075865 0.55903272 0.55761024 0.56282598\n",
      " 0.55429113 0.5637743  0.56235183 0.55429113 0.5637743  0.56235183\n",
      " 0.54812707 0.55476529 0.55713608        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.55903272 0.56851588 0.57610242 0.54954955 0.56140351 0.56187767\n",
      " 0.52157421 0.52821242 0.52441916 0.55618777 0.55476529 0.55429113\n",
      " 0.54148886 0.5523945  0.54907539 0.51209104 0.5182551  0.51920341\n",
      " 0.52015173 0.53010906 0.52726411 0.52015173 0.53010906 0.52726411\n",
      " 0.51778094 0.51778094 0.52015173 0.55903272 0.56851588 0.57610242\n",
      " 0.54954955 0.56140351 0.56187767 0.52157421 0.52821242 0.52441916\n",
      " 0.55618777 0.55476529 0.55429113 0.54148886 0.5523945  0.54907539\n",
      " 0.51209104 0.5182551  0.51920341 0.52015173 0.53010906 0.52726411\n",
      " 0.52015173 0.53010906 0.52726411 0.51778094 0.51778094 0.52015173\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.63252726 0.63584637 0.63632053\n",
      " 0.60312945 0.61593172 0.61877667 0.56140351 0.56472262 0.5694642\n",
      " 0.60550024 0.61071598 0.6116643  0.59554291 0.60265529 0.60312945\n",
      " 0.55618777 0.55855856 0.56187767 0.55618777 0.56614509 0.56567093\n",
      " 0.55618777 0.56614509 0.56567093 0.54623044 0.55097202 0.55523945\n",
      " 0.63252726 0.63584637 0.63632053 0.60312945 0.61593172 0.61877667\n",
      " 0.56140351 0.56472262 0.5694642  0.60550024 0.61071598 0.6116643\n",
      " 0.59554291 0.60265529 0.60312945 0.55618777 0.55855856 0.56187767\n",
      " 0.55618777 0.56614509 0.56567093 0.55618777 0.56614509 0.56567093\n",
      " 0.54623044 0.55097202 0.55523945        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.62446657 0.63300142 0.63394974 0.60692271 0.61688004 0.61830251\n",
      " 0.56330014 0.56614509 0.56804173 0.60455192 0.60787103 0.61308677\n",
      " 0.59696539 0.59696539 0.60123281 0.55903272 0.55761024 0.56282598\n",
      " 0.55429113 0.5637743  0.56235183 0.55429113 0.5637743  0.56235183\n",
      " 0.54812707 0.55476529 0.55713608 0.62446657 0.63300142 0.63394974\n",
      " 0.60692271 0.61688004 0.61830251 0.56330014 0.56614509 0.56804173\n",
      " 0.60455192 0.60787103 0.61308677 0.59696539 0.59696539 0.60123281\n",
      " 0.55903272 0.55761024 0.56282598 0.55429113 0.5637743  0.56235183\n",
      " 0.55429113 0.5637743  0.56235183 0.54812707 0.55476529 0.55713608\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.6230441  0.63205311 0.63584637\n",
      " 0.60739687 0.61640588 0.61830251 0.56330014 0.56614509 0.56804173\n",
      " 0.60312945 0.60787103 0.61308677 0.59601707 0.59649123 0.60075865\n",
      " 0.55903272 0.55761024 0.56282598 0.55429113 0.5637743  0.56235183\n",
      " 0.55429113 0.5637743  0.56235183 0.54812707 0.55476529 0.55713608\n",
      " 0.6230441  0.63205311 0.63584637 0.60739687 0.61640588 0.61830251\n",
      " 0.56330014 0.56614509 0.56804173 0.60312945 0.60787103 0.61308677\n",
      " 0.59601707 0.59649123 0.60075865 0.55903272 0.55761024 0.56282598\n",
      " 0.55429113 0.5637743  0.56235183 0.55429113 0.5637743  0.56235183\n",
      " 0.54812707 0.55476529 0.55713608        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.6230441  0.63205311 0.63584637 0.60739687 0.61640588 0.61830251\n",
      " 0.56330014 0.56614509 0.56804173 0.60312945 0.60787103 0.61308677\n",
      " 0.59601707 0.59649123 0.60075865 0.55903272 0.55761024 0.56282598\n",
      " 0.55429113 0.5637743  0.56235183 0.55429113 0.5637743  0.56235183\n",
      " 0.54812707 0.55476529 0.55713608 0.6230441  0.63205311 0.63584637\n",
      " 0.60739687 0.61640588 0.61830251 0.56330014 0.56614509 0.56804173\n",
      " 0.60312945 0.60787103 0.61308677 0.59601707 0.59649123 0.60075865\n",
      " 0.55903272 0.55761024 0.56282598 0.55429113 0.5637743  0.56235183\n",
      " 0.55429113 0.5637743  0.56235183 0.54812707 0.55476529 0.55713608\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.63489806 0.63394974 0.63205311\n",
      " 0.62446657 0.62731152 0.62636321 0.60407776 0.60739687 0.60550024\n",
      " 0.628734   0.62588905 0.62446657 0.61545756 0.62494073 0.6230441\n",
      " 0.58843054 0.58937885 0.58748222 0.58368895 0.58843054 0.5865339\n",
      " 0.58368895 0.58843054 0.5865339  0.57989569 0.57942153 0.57799905\n",
      " 0.63489806 0.63394974 0.63205311 0.62446657 0.62731152 0.62636321\n",
      " 0.60407776 0.60739687 0.60550024 0.628734   0.62588905 0.62446657\n",
      " 0.61545756 0.62494073 0.6230441  0.58843054 0.58937885 0.58748222\n",
      " 0.58368895 0.58843054 0.5865339  0.58368895 0.58843054 0.5865339\n",
      " 0.57989569 0.57942153 0.57799905        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.58179232 0.58131816 0.57989569 0.56235183 0.56424846 0.56140351\n",
      " 0.53105737 0.54054054 0.53911807 0.55950688 0.56661925 0.56330014\n",
      " 0.55903272 0.55998103 0.55950688 0.5296349  0.53674727 0.53247985\n",
      " 0.53485064 0.53153153 0.53437648 0.53485064 0.53153153 0.53437648\n",
      " 0.52726411 0.53200569 0.53010906 0.58179232 0.58131816 0.57989569\n",
      " 0.56235183 0.56424846 0.56140351 0.53105737 0.54054054 0.53911807\n",
      " 0.55950688 0.56661925 0.56330014 0.55903272 0.55998103 0.55950688\n",
      " 0.5296349  0.53674727 0.53247985 0.53485064 0.53153153 0.53437648\n",
      " 0.53485064 0.53153153 0.53437648 0.52726411 0.53200569 0.53010906\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.6344239  0.63394974 0.63489806\n",
      " 0.62731152 0.63157895 0.63110479 0.6036036  0.60123281 0.60170697\n",
      " 0.62588905 0.62636321 0.62399241 0.61261261 0.62778568 0.62636321\n",
      " 0.59080133 0.59174964 0.58890469 0.58226648 0.58700806 0.58368895\n",
      " 0.58226648 0.58700806 0.58368895 0.57752489 0.57610242 0.57610242\n",
      " 0.6344239  0.63394974 0.63489806 0.62731152 0.63157895 0.63110479\n",
      " 0.6036036  0.60123281 0.60170697 0.62588905 0.62636321 0.62399241\n",
      " 0.61261261 0.62778568 0.62636321 0.59080133 0.59174964 0.58890469\n",
      " 0.58226648 0.58700806 0.58368895 0.58226648 0.58700806 0.58368895\n",
      " 0.57752489 0.57610242 0.57610242        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.63347558 0.63537221 0.63205311 0.62446657 0.62778568 0.62541489\n",
      " 0.60407776 0.60739687 0.60550024 0.628734   0.62588905 0.62446657\n",
      " 0.61545756 0.62494073 0.62351826 0.58843054 0.58937885 0.58795638\n",
      " 0.58368895 0.58843054 0.5865339  0.58368895 0.58843054 0.5865339\n",
      " 0.57989569 0.57942153 0.57799905 0.63347558 0.63537221 0.63205311\n",
      " 0.62446657 0.62778568 0.62541489 0.60407776 0.60739687 0.60550024\n",
      " 0.628734   0.62588905 0.62446657 0.61545756 0.62494073 0.62351826\n",
      " 0.58843054 0.58937885 0.58795638 0.58368895 0.58843054 0.5865339\n",
      " 0.58368895 0.58843054 0.5865339  0.57989569 0.57942153 0.57799905\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.63489806 0.63394974 0.63205311\n",
      " 0.62446657 0.62731152 0.62636321 0.60407776 0.60739687 0.60550024\n",
      " 0.628734   0.62588905 0.62446657 0.61545756 0.62494073 0.6230441\n",
      " 0.58843054 0.58937885 0.58748222 0.58368895 0.58843054 0.5865339\n",
      " 0.58368895 0.58843054 0.5865339  0.57989569 0.57942153 0.57799905\n",
      " 0.63489806 0.63394974 0.63205311 0.62446657 0.62731152 0.62636321\n",
      " 0.60407776 0.60739687 0.60550024 0.628734   0.62588905 0.62446657\n",
      " 0.61545756 0.62494073 0.6230441  0.58843054 0.58937885 0.58748222\n",
      " 0.58368895 0.58843054 0.5865339  0.58368895 0.58843054 0.5865339\n",
      " 0.57989569 0.57942153 0.57799905        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.63489806 0.63394974 0.63205311 0.62446657 0.62731152 0.62636321\n",
      " 0.60407776 0.60739687 0.60550024 0.628734   0.62588905 0.62446657\n",
      " 0.61545756 0.62494073 0.6230441  0.58843054 0.58937885 0.58748222\n",
      " 0.58368895 0.58843054 0.5865339  0.58368895 0.58843054 0.5865339\n",
      " 0.57989569 0.57942153 0.57799905 0.63489806 0.63394974 0.63205311\n",
      " 0.62446657 0.62731152 0.62636321 0.60407776 0.60739687 0.60550024\n",
      " 0.628734   0.62588905 0.62446657 0.61545756 0.62494073 0.6230441\n",
      " 0.58843054 0.58937885 0.58748222 0.58368895 0.58843054 0.5865339\n",
      " 0.58368895 0.58843054 0.5865339  0.57989569 0.57942153 0.57799905]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'bootstrap': True, 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "Validation Accuracy: 0.7026515151515151\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 4: Model Selection and Hyperparameter Tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth': [None, 10, 20, 30, 50, 100],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters from grid search\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best parameters found: \", best_params)\n",
    "\n",
    "# Step 5: Model Evaluation with the best parameters\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_pred = best_rf.predict(X_val)\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(\"Validation Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7026515151515151\n",
      "Validation Precision: 0.7352211761302672\n",
      "Validation Recall: 0.7026515151515151\n",
      "Validation F1-Score: 0.6932829564632985\n",
      "Confusion Matrix:\n",
      " [[8 0 2 ... 0 0 0]\n",
      " [0 5 0 ... 0 0 0]\n",
      " [0 0 5 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 2 0 0]\n",
      " [0 0 0 ... 0 3 0]\n",
      " [0 0 0 ... 0 0 2]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          20       0.53      0.73      0.62        11\n",
      "          27       0.71      0.83      0.77         6\n",
      "          34       0.62      1.00      0.77         5\n",
      "          41       1.00      0.88      0.93         8\n",
      "          48       0.75      0.38      0.50         8\n",
      "          55       0.67      0.75      0.71         8\n",
      "          62       0.86      0.60      0.71        10\n",
      "          69       1.00      1.00      1.00         5\n",
      "          76       0.60      0.86      0.71         7\n",
      "          90       0.00      0.00      0.00         1\n",
      "         118       1.00      0.33      0.50         3\n",
      "         146       0.67      1.00      0.80         2\n",
      "         188       0.75      0.75      0.75         4\n",
      "         195       0.00      0.00      0.00         1\n",
      "         202       0.80      0.80      0.80        10\n",
      "         209       0.00      0.00      0.00         0\n",
      "         216       0.67      0.29      0.40         7\n",
      "         223       1.00      0.50      0.67         2\n",
      "         237       0.00      0.00      0.00         0\n",
      "         251       1.00      1.00      1.00         7\n",
      "         272       1.00      1.00      1.00         1\n",
      "         293       0.33      1.00      0.50         1\n",
      "         300       1.00      1.00      1.00         1\n",
      "         769       1.00      1.00      1.00         1\n",
      "        1490       1.00      1.00      1.00         2\n",
      "        6698       1.00      0.89      0.94         9\n",
      "        6838       0.00      0.00      0.00         0\n",
      "        7258       1.00      0.83      0.91         6\n",
      "        7573       1.00      1.00      1.00         2\n",
      "        7608       0.50      0.50      0.50         2\n",
      "        7727       0.67      1.00      0.80         4\n",
      "        8056       0.00      0.00      0.00         2\n",
      "        8182       0.80      1.00      0.89         4\n",
      "        8301       0.50      0.50      0.50         4\n",
      "        8700       0.50      1.00      0.67         1\n",
      "        8917       1.00      0.86      0.92         7\n",
      "        8987       0.50      0.75      0.60         4\n",
      "        9701       0.83      0.50      0.62        10\n",
      "        9876       0.77      1.00      0.87        10\n",
      "        9967       0.00      0.00      0.00         5\n",
      "       10366       0.75      1.00      0.86         6\n",
      "       10576       0.67      0.67      0.67         3\n",
      "       10618       0.80      0.50      0.62         8\n",
      "       11157       0.00      0.00      0.00         1\n",
      "       11220       0.80      1.00      0.89         4\n",
      "       11283       0.20      1.00      0.33         1\n",
      "       11374       1.00      0.50      0.67         2\n",
      "       11591       0.75      1.00      0.86         3\n",
      "       11857       1.00      1.00      1.00         1\n",
      "       12046       1.00      0.67      0.80         3\n",
      "       12389       1.00      1.00      1.00         1\n",
      "       12473       0.50      1.00      0.67         1\n",
      "       12634       1.00      0.50      0.67         2\n",
      "       12669       0.50      1.00      0.67         1\n",
      "       12718       1.00      1.00      1.00         1\n",
      "       13131       1.00      0.75      0.86         4\n",
      "       13166       1.00      1.00      1.00         1\n",
      "       13397       0.00      0.00      0.00         1\n",
      "       13474       1.00      0.67      0.80         3\n",
      "       13957       0.00      0.00      0.00         1\n",
      "       13971       0.00      0.00      0.00         1\n",
      "       14286       0.00      0.00      0.00         1\n",
      "       14454       0.71      0.71      0.71         7\n",
      "       14860       0.67      0.80      0.73         5\n",
      "       14965       0.00      0.00      0.00         1\n",
      "       15203       0.00      0.00      0.00         1\n",
      "       15301       1.00      1.00      1.00         1\n",
      "       15413       0.33      0.17      0.22         6\n",
      "       15497       0.00      0.00      0.00         3\n",
      "       15623       0.27      1.00      0.43         3\n",
      "       17583       0.00      0.00      0.00         0\n",
      "       17653       1.00      0.33      0.50         6\n",
      "       17744       1.00      0.50      0.67         2\n",
      "       17982       0.75      1.00      0.86         3\n",
      "       18360       0.00      0.00      0.00         0\n",
      "       18570       1.00      1.00      1.00         5\n",
      "       22497       1.00      0.50      0.67         2\n",
      "       22763       0.50      0.50      0.50         2\n",
      "       22784       1.00      1.00      1.00         1\n",
      "       23113       0.25      0.33      0.29         3\n",
      "       23316       1.00      1.00      1.00         2\n",
      "       23750       0.67      1.00      0.80         2\n",
      "       23841       0.00      0.00      0.00         2\n",
      "       23869       1.00      1.00      1.00         3\n",
      "       28594       1.00      1.00      1.00         1\n",
      "       30393       0.60      0.75      0.67        12\n",
      "       30400       0.75      0.33      0.46         9\n",
      "       30407       0.75      0.43      0.55         7\n",
      "       30414       0.83      0.71      0.77         7\n",
      "       30421       0.75      1.00      0.86         3\n",
      "       30428       0.44      0.80      0.57         5\n",
      "       30435       1.00      1.00      1.00         5\n",
      "       33914       1.00      1.00      1.00         2\n",
      "       33921       0.75      0.50      0.60         6\n",
      "       33928       0.67      1.00      0.80         2\n",
      "       33935       0.91      1.00      0.95        10\n",
      "       33942       0.33      0.25      0.29         4\n",
      "       33949       0.71      0.83      0.77         6\n",
      "       33956       0.82      1.00      0.90         9\n",
      "       33963       0.00      0.00      0.00         2\n",
      "       35790       0.20      1.00      0.33         1\n",
      "       36014       0.75      0.60      0.67         5\n",
      "       36070       0.57      0.67      0.62         6\n",
      "       36084       1.00      0.50      0.67         2\n",
      "       36098       0.50      0.67      0.57         3\n",
      "       36112       0.50      0.50      0.50         4\n",
      "       36126       0.62      0.56      0.59         9\n",
      "       38814       0.00      0.00      0.00         1\n",
      "       39416       1.00      1.00      1.00         1\n",
      "       40298       1.00      1.00      1.00         5\n",
      "       40424       1.00      1.00      1.00         2\n",
      "       40452       1.00      1.00      1.00         1\n",
      "       40550       0.50      0.80      0.62         5\n",
      "       40564       1.00      0.33      0.50         3\n",
      "       40578       0.50      0.75      0.60         4\n",
      "       40592       1.00      0.50      0.67         2\n",
      "       40606       1.00      0.67      0.80         3\n",
      "       42573       1.00      1.00      1.00         2\n",
      "       44904       1.00      0.90      0.95        10\n",
      "       45072       1.00      0.33      0.50         3\n",
      "       45940       0.00      0.00      0.00         1\n",
      "       45947       1.00      1.00      1.00         1\n",
      "       45954       0.00      0.00      0.00         1\n",
      "       46731       0.67      0.67      0.67         3\n",
      "       46738       0.80      0.57      0.67         7\n",
      "       46745       1.00      0.50      0.67         2\n",
      "       46752       0.00      0.00      0.00         0\n",
      "       46759       0.80      1.00      0.89         4\n",
      "       46766       0.00      0.00      0.00         1\n",
      "       46773       0.67      0.67      0.67         3\n",
      "       46780       1.00      0.80      0.89         5\n",
      "       47480       0.00      0.00      0.00         1\n",
      "       47487       0.67      1.00      0.80         2\n",
      "       47494       0.00      0.00      0.00         2\n",
      "       47501       0.71      1.00      0.83         5\n",
      "       47508       0.80      0.67      0.73         6\n",
      "       47529       1.00      0.60      0.75         5\n",
      "       48334       1.00      1.00      1.00         3\n",
      "       48341       0.67      1.00      0.80         4\n",
      "       48733       0.00      0.00      0.00         0\n",
      "       48922       0.67      1.00      0.80         2\n",
      "       48929       0.60      1.00      0.75         3\n",
      "       48936       1.00      0.33      0.50         3\n",
      "       48943       0.50      0.25      0.33         4\n",
      "       48950       1.00      1.00      1.00         2\n",
      "       49650       1.00      1.00      1.00         3\n",
      "       49657       0.67      1.00      0.80         2\n",
      "\n",
      "    accuracy                           0.70       528\n",
      "   macro avg       0.65      0.64      0.61       528\n",
      "weighted avg       0.74      0.70      0.69       528\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rahul.k.a.jha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\rahul.k.a.jha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\rahul.k.a.jha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\rahul.k.a.jha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\rahul.k.a.jha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\rahul.k.a.jha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\rahul.k.a.jha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\rahul.k.a.jha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "# Step 5: Model Evaluation with the best parameters\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_pred = best_rf.predict(X_val)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(\"Validation Accuracy:\", accuracy)\n",
    "\n",
    "# Precision\n",
    "precision = precision_score(y_val, y_pred, average='weighted')  # Use 'weighted' to account for class imbalance\n",
    "print(\"Validation Precision:\", precision)\n",
    "\n",
    "# Recall\n",
    "recall = recall_score(y_val, y_pred, average='weighted')\n",
    "print(\"Validation Recall:\", recall)\n",
    "\n",
    "# F1-Score\n",
    "f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "print(\"Validation F1-Score:\", f1)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_val, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "# Classification Report\n",
    "class_report = classification_report(y_val, y_pred)\n",
    "print(\"Classification Report:\\n\", class_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the primary train data\n",
    "test_path = './mergedtest.csv'  # Replace with the actual file path\n",
    "test_data = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [match id, team1, team1_id, team2, team2_id, toss winner, toss decision, venue, city, lighting, series_name, season, ground_id, team_count_50runs_last15, team_winp_last5, team1only_avg_runs_last15, team1_winp_team2_last15, ground_avg_runs_last15, dataset_type]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print(test_data.head(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the columns used for prediction\n",
    "predict_columns = [\n",
    "    'match id', 'team1', 'team1_id', 'team2', 'team2_id', 'toss winner', 'toss decision', 'venue', 'city', \n",
    "    'lighting', 'series_name', 'season', 'ground_id', \n",
    "    'team_count_50runs_last15', 'team_winp_last5', 'team1only_avg_runs_last15', \n",
    "    'team1_winp_team2_last15', 'ground_avg_runs_last15'\n",
    "]\n",
    "\n",
    "# Ensure 'dataset_type' is not in predict_columns\n",
    "predict_columns = [col for col in predict_columns if col != 'dataset_type']\n",
    "\n",
    "# Prepare the data for prediction (excluding 'dataset_type')\n",
    "X_test = test_data[predict_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ensure the test dataset has the same feature columns as the training data\n",
    "# X_test = test_data[X_train.columns]\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = best_rf.predict(X_test)\n",
    "prediction_scores = best_rf.predict_proba(X_test).max(axis=1)  # Get the max probability for the predicted class\n",
    "\n",
    "# Output the predictions\n",
    "# Prepare DataFrame\n",
    "dep_var = pd.DataFrame({\n",
    "    'match id': test_data['match id'],\n",
    "    'dataset_type': test_data['dataset_type'],\n",
    "    'winner_id':  predictions,\n",
    "})\n",
    "\n",
    "dep_var.to_csv('./dep_var.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primary Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Get top 10 features and their values for each prediction\n",
    "top_10_features = np.argsort(best_rf.feature_importances_)[-10:][::-1]\n",
    "\n",
    "# Assuming `test_data` is your test DataFrame\n",
    "test_data['win_pred_team_id'] = predictions\n",
    "test_data['win_pred_score'] = prediction_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary submission file created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Model and hyperparameters details\n",
    "train_algorithm = 'RandomForest'\n",
    "is_ensemble = 'no'\n",
    "train_hps_trees = 300  # Example value\n",
    "train_hps_depth = 20  # Example value\n",
    "train_hps_lr = 0  # Example value for non-ensemble model\n",
    "\n",
    "# Prepare DataFrame\n",
    "primary_submission = pd.DataFrame({\n",
    "    'match id': test_data['match id'],\n",
    "    'dataset_type': test_data['dataset_type'],\n",
    "    'win_pred_team_id': test_data['win_pred_team_id'],\n",
    "    'win_pred_score': test_data['win_pred_score'],\n",
    "    'train_algorithm': train_algorithm,\n",
    "    'is_ensemble': is_ensemble,\n",
    "    'train_hps_trees': train_hps_trees,\n",
    "    'train_hps_depth': train_hps_depth,\n",
    "    'train_hps_lr': train_hps_lr,\n",
    "})\n",
    "\n",
    "# Add top 10 feature values to the DataFrame\n",
    "for i, feat_idx in enumerate(top_10_features):\n",
    "    primary_submission[f'indep_feat_id{i+1}'] = X_test.iloc[:, feat_idx].values\n",
    "\n",
    "# Save to CSV\n",
    "primary_submission.to_csv('primary_submission.csv', index=False)\n",
    "\n",
    "print(\"Primary submission file created successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondary Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Secondary submission file created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Get feature importances\n",
    "feature_importances = best_rf.feature_importances_\n",
    "\n",
    "# Create a DataFrame for feature importances\n",
    "features_df = pd.DataFrame({\n",
    "    'feat_id': range(1, len(X.columns) + 1),\n",
    "    'feat_name': X.columns,\n",
    "    'model_feat_imp_train': feature_importances\n",
    "})\n",
    "\n",
    "# Rank the features by importance\n",
    "features_df['feat_rank_train'] = features_df['model_feat_imp_train'].rank(ascending=False).astype(int)\n",
    "\n",
    "# Add a placeholder for feature descriptions (you should replace this with actual descriptions)\n",
    "features_df['feat_description'] = \"Description of \" + features_df['feat_name']\n",
    "\n",
    "# Sort by feature importance rank\n",
    "features_df = features_df.sort_values(by='feat_rank_train')\n",
    "\n",
    "# Save to CSV\n",
    "features_df.to_csv('secondary_submission.csv', index=False)\n",
    "\n",
    "print(\"Secondary submission file created successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
